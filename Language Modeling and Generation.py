import re
import sys
import random
import math
import collections
from collections import defaultdict


class Ngram_Language_Model:
    """
        The class implements a Markov Language Model that learns a language model from a given text.
        It supoprts language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """


    def __init__(self, n=3, chars=False):
        """
        Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False.
        Explanation:
            In this function i define a dictionary of ngram and dictionary of dictionaries of all grams from 1 to n.
            In addition in case that try to define model language with n<1 i change the n to be 3.
        """

        self.n = n
        self.model_dict = defaultdict(int)  # a dictionary of the form {ngram:count}, holding counts of all ngrams in the specified text.
        self.model_dict_of_dicts = None # a dictionary of ngrams dictionaries.
        self.chars=chars
        self.dict_of_vocabulary_count = None
        #If n < 1 , Ngram model Will not be able to work, so in such a case I will change n to be the default value, 3
        if n < 1 :
            self.n = 3

    def build_model(self, text):
        """
            This function build the model.

            Args:
                text (str): the text to construct the model from.

            Explanation:
                This function populate a dictionary that count all ngrams in the specified text.
                Moreover this function populate a dictionary of dictionaries. Each one of the dictionaries is counting all
                grams from 1 to n.
                In addition it makes a dictionary for count the vocabulary for each n (will used for smooth)
        """
        dict_of_dicts = {}
        dict_of_vocabulary_count = {}
        if self.chars == False: #in case of words
            tokens = [token for token in text.split(" ") if token != ""] #create a list of all tokens
            for i in range(1, self.n + 1 ):
                ngrams = zip(*[tokens[i:] for i in range(i)])  # Goes over the tokens and attaches each series of n words
                ngrams = [" ".join(ngram) for ngram in ngrams]  # Makes a space between the words
                dict_of_dicts[i] = collections.Counter(ngrams)  # Counts the number of times a particular ngram appears and populates a dictionary
                dict_of_vocabulary_count[i] = len(dict_of_dicts[i].items())  # Counts the number of distinct words in each ngram
            for i in range(1, self.n):
                is_not_context = tokens[len(tokens) - i:]
                is_not_context = ' '.join(is_not_context)
                dict_of_dicts[i][is_not_context] -= 1
            self.model_dict_of_dicts = dict_of_dicts
            self.model_dict = defaultdict(int,dict_of_dicts[self.n])
            self.dict_of_vocabulary_count = dict_of_vocabulary_count
        else: #in case of chars
            for i in range(1, self.n + 1):
                ngrams = ["".join(j) for j in zip(*[text[i:] for i in range(i)])]
                dict_of_dicts[i] = collections.Counter(ngrams)  # Counts the number of times a particular ngram appears and populates a dictionary
                dict_of_vocabulary_count[i] = len(dict_of_dicts[i].items())  # Counts the number of distinct words in each ngram
            self.model_dict_of_dicts = dict_of_dicts
            self.model_dict = defaultdict(int, dict_of_dicts[self.n])
            self.dict_of_vocabulary_count = dict_of_vocabulary_count


    def get_model_dictionary(self):
        """
        Returns the dictionary class object
        """
        return self.model_dict

    def get_model_window_size(self):
        """
        Returning the size of the context window (the n in "n-gram")
        """
        return self.n


    def generate(self, context=None, n=20):
        """
        Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context will be sampled
        from the models' contexts distribution. Generation will stop before the n'th word if the
        contexts are exhausted (if no more context in the sentence).
        If the length of the specified context exceeds (or equal to) the specified n,
        the method will return the a prefix of length n of the specified context.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

            Explanation:
            if there is no context or the context is not in length of (self.n - 1 ) the function will call to complete_
            or_create function that will create a context or complete it to n-1 length and then will continue regular.
            This function will functions that will help to generate sentence according to the case (words or chars).
            If the first context is OOV the function know to generate sentence too. the generate will stop only if there
            is no more context ךater in the sentence.
        """
        get_context = True # in case that we get context
        generated_text = ""
        if (self.chars == False):
            first_tokens = [token for token in context.split(" ") if token != ""]
            start_context_len = len(first_tokens)
        else:  # in case of chars
            start_context_len = len(context)
        if start_context_len >= n: # this part will return the context in length n when the context is equal or exceeds n.
            if start_context_len == n:
                return context
            else:
                return context.rsplit(' ', start_context_len-n)[0]
        # if there is no context
        elif context is None or context == "": #create a context when not get one.
            context = ""
            context = self.create_or_complete_context(context, get_context) # the function know when to create or complete.
            get_context = False
        # if the context is short
        elif start_context_len < (self.n-1):
            context = normalize_text(context)
            context = self.create_or_complete_context(context, get_context)
        # if context is in length of n-1
        else:
            context = normalize_text(context)

        generated_text = generated_text + context

        if (self.chars == False):
            tokens = [token for token in generated_text.split(" ") if token != ""]
            num_of_tokens_in_context = len(tokens)
        else: #case of chars
            num_of_tokens_in_context = len(context)

        generated_text_len = num_of_tokens_in_context

        while generated_text_len < n:
            if self.chars == False:
                next_generate = self.generate_word(generated_text, generated_text_len ,get_context )
                if next_generate == True:
                    break
                generated_text = generated_text + " " + next_generate
                generated_text_len += 1
            else:
                next_generate = self.generate_char(generated_text, generated_text_len ,get_context )
                if next_generate == True:
                    break
                generated_text = generated_text + next_generate
                generated_text_len += 1

        return generated_text

    def create_or_complete_context(self, context, get_context):
        """
            This fucntion will return a context to start from.

            Args:
                context (str): the context that we get or not get.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String, Context.

            Explanation:
                There is two option for this function: 
                1.to create a context when we dont get one , in this case the function
                 will sample a context of size n by distribution of ngram.
                2. complete a context when we get a short one. in this case the function will complete the context 
                    until the context will be in length of n-1.
                
        """
        tokens = [token for token in context.split(" ") if token != ""]
        if self.chars == False:
            context_len = len(tokens)
        else:
            context_len = len(context)
        dist_dict = {} # for complete
        current_context = context

        if context_len == 0:  # need to create context (work for chars and for words)
            if self.n > 1:
                dict = self.model_dict_of_dicts[self.n - 1]
            else:
                dict = self.model_dict_of_dicts[self.n]
            sum_val = sum(dict.values())
            for item in dict.keys():
                prob = dict[item] / sum_val
                dist_dict[item] = prob
            sample_context = random.choices(list(dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)
            return sample_context[0]

        else:
            while context_len < (self.n - 1):
                if self.chars == False:  # in case of words
                    generated_word = self.generate_word(current_context, context_len, get_context)
                    current_context = current_context + " " + generated_word
                    context_len += 1
                else: #case of chars
                    generated_char = self.generate_char(current_context, context_len, get_context)
                    current_context = current_context + generated_char
                    context_len += 1
            return current_context

    def generate_word(self, genereted_text, context_len, get_context):
        """
            This fucntion will return the n+1 word (generate word).

            Args:
                genereted_text (str): the text that generated until now.
                context_len(int): the length of the context to generate from.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String , word to be generate.

            Explanation:
                The function get context and according to the length of the context know how to generate.
                In case of OVV (not context) we have 2 cases: 
                1.we get a context in the start but the context OOV so the function still generate a word because i want 
                    to generate text.
                2. the other case is a case of OOV after we start generate and in this case the generate will be stoped.
            
        """
        OOV_flag = False
        count_valuse = 0  # need it if OVV
        dist_dict = {}
        candidates_dict = {}
        context = genereted_text
        if self.n > 1:
            if self.n <= context_len:  # make a context in length of (n-1) (n is of the ngram)
                words_for_context = context.split()[-(self.n - 1):]
                context = ""
                for i in range(0, self.n - 1):
                    context = context + words_for_context[i] + " "
                context = context.rstrip()
            tokens = [token for token in context.split(" ") if token != ""]
            cur_context_len = len(tokens)
            candidates_dict = self.candidates(context, context_len)

        else:  # in case that self.n = 1
            OOV_flag = True
            cur_context_len = 1
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # in case of OOV
        if len(candidates_dict) == 0 and get_context == True:
            OOV_flag = True
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()

        if len(candidates_dict) == 0:  # stop when no more context
            stop_generate_flag = True
            return stop_generate_flag
        # calculate the probability
        for candidate in candidates_dict.keys():
            context_count = candidates_dict[candidate]
            if OOV_flag == True:
                prob = self.model_dict_of_dicts[1][candidate] / sum(count_valuse)
                dist_dict[candidate] = prob
            else:
                dist_dict[candidate] = self.calc_prob_for_generate(candidate, context, context_count, cur_context_len)
        sample = random.choices(list(dist_dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)

        return sample[0]

    def generate_char(self, genereted_text, context_len, get_context):
        """
            This fucntion will return the n+1 char (generate word).

            Args:
                genereted_text (str): the text that generated until now.
                context_len(int): the length of the context to generate from.
                get_context (boolean): tell us if we get context from start or not.

            Return:
                String , char to be generate.

            Explanation:
                the explanation is same like in words.

        """
        OOV_flag = False
        count_valuse = 0  # need it if OVV
        dist_dict = {}
        candidates_dict = {}
        context = genereted_text
        if self.n > 1:
            if self.n <= context_len:
                context = context[context_len - (self.n - 1): context_len] # make a context in length of (n-1) (n is of the ngram)
            cur_context_len = len(context)
            candidates_dict = self.candidates(context, context_len)

        else:  # in case that self.n = 1
            OOV_flag = True
            cur_context_len = 1
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # in case of OOV
        if len(candidates_dict) == 0 and get_context == True:
            OOV_flag = True
            candidates_dict = self.model_dict_of_dicts[1]
            count_valuse = self.model_dict_of_dicts[1].values()
        # stop when no more context
        if len(candidates_dict) == 0:
            stop_generate_flag = True
            return stop_generate_flag
        # calculate the probability
        for candidate in candidates_dict.keys():
            context_count = candidates_dict[candidate]
            if OOV_flag == True:
                prob = self.model_dict_of_dicts[1][candidate] / sum(count_valuse)
                dist_dict[candidate] = prob
            else:
                dist_dict[candidate] = self.calc_prob_for_generate(candidate, context, context_count, cur_context_len)
        sample = random.choices(list(dist_dict.keys()), weights=dist_dict.values(), cum_weights=None, k=1)

        return sample[0]

    def candidates(self, context , num_of_words_in_sen):
        """
            This function will return a dictionary of optional words/chars to be generate
            
            Args: 
                context(str) : context to check which words/chars can be candidate
                num_of_words_in_sen(int) : the length of the sentence that already generated

            Return:
                dictionary of candidates.

            Explanation:
                This function get a context and the length of the sentence until now and make a dictionary of all words
                that can be generate from this context (or empty dictionary for OOV context) and the number of time that
                the context appear ( will help later to calculate the probability).

        """
        check_in={}
        cand_dict = {}
        context_count = 0
        tokens = [token for token in context.split(" ") if token != ""]
        if self.chars == False:
            context_len = len(tokens)
        else:
            context_len = len(context)
        if context_len == (self.n-1):
            check_in = self.model_dict
        else:
            check_in = self.model_dict_of_dicts[context_len+1]
        if self.chars == False:
            for item in check_in.keys():
                if self.n > num_of_words_in_sen:
                    cand_help = item.rsplit(' ', self.n - num_of_words_in_sen)
                    if cand_help[0] == context:
                        context_count = context_count + check_in[item]
                        cand_dict[cand_help[1]] = 0
                else:
                    cand_help = item.rsplit(' ', 1)
                    if cand_help[0] == context:
                        context_count = context_count + self.model_dict[item]
                        cand_dict[cand_help[1]] = 0
            for cand in cand_dict.keys():
                cand_dict[cand] = context_count
        else:
            for item in check_in.keys():
                cand_help = item[(len(item)-(self.n)):len(item)-1]
                if cand_help == context:
                    context_count = context_count + check_in[item]
                    cand_dict[item[-1]] = 0
            for cand in cand_dict.keys():
                cand_dict[cand] = context_count

        return cand_dict


    def calc_prob_for_generate(self, word, context , context_count , context_len):
        """
            This function will return the probability of a word/char to be after a context.

            Args:
                word (str): word or char to calculate probability for.
                context(str) : context that help to calculate probability.
                context_count(int) : the number of times that the context was appeared before any words/chars
                context_len(int) : lenght of the context

            Return:
                    Float - probability
            Explanation:
                calculate probability for specific word or char to be after a context.

        """

        "Probability of `word` to be after context."
        if self.chars == False:
            word_with_context = context + " " + word
        else:
            word_with_context = context + word
        count_of_word_after_context = self.model_dict_of_dicts[context_len+1][word_with_context]
        return count_of_word_after_context / context_count

    def evaluate(self, text):
        """
           Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing will be applied if necessary.

           The function will normalize the text and create ngrams dictionary , then will check if need to calculate
           probability by using smooth or not and return the log-likelihood prob.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.

            Explanation:
                The function will create ngrams from the text and in addition will create ngrams from 1 to (n-1)gram
                to calculate the probabilities for the starts words too.
                This function use calculate_prob_for_evaluate or smooth according to the situation to calculate the
                total probability for text to be product from the model.
        """

        log_prob = 0
        smooth_flag = False # tell us if we need to calculate by using laplace smooth or not.
        if self.chars == False:
            norm_text = normalize_text(text)
            tokens = [token for token in norm_text.split(" ") if token != ""]  # create a list of all tokens
            if len(tokens) >= self.n:
                ngrams_from_text = zip(*[tokens[i:] for i in range(self.n)])
            else:
                ngrams_from_text = zip(*[tokens[i:] for i in range(len(tokens))]) # for evaluate when text < n
            ngrams_from_text = [" ".join(ngram) for ngram in ngrams_from_text]  # Makes a space between the words

            #spacial case, when n = 1
            if self.n == 1 :
                prob = (self.model_dict_of_dicts[1][norm_text]) / (
                    sum(self.model_dict_of_dicts[1].values()))
                if prob == 0:
                    return math.log(1 / (sum(self.model_dict_of_dicts[1].values()))) #smooth
                return math.log(prob)
            #we want to calc the probability to generate the start of the sentence before the ngram, this code
            #add the start of the sentence until the ngram to the list and calculate the prob too.
            if self.n - 1 <= len(tokens):
                ran = self.n
            else:
                ran= len(tokens)
                #add the words until (n-1)gram to the dict to calculate the probability of them too.
            temp_list = []
            for j in range (1, ran):
                ngram_minus_1_dict = zip(*[tokens[i:] for i in range(j)])
                ngram_minus_1_dict = [" ".join(ngram) for ngram in ngram_minus_1_dict]
                temp_list.append(ngram_minus_1_dict[0])
            ngrams_from_text += temp_list

        else:
            nt = normalize_text(text, True)
            ngrams_from_text = [nt[i:i + self.n] for i in range(len(nt) - self.n + 1)]

            temp_list = []
            for j in range(1, self.n):
                ngram_minus_1_dict = [text[i:i + j] for i in range(len(text) - j + 1)]
                temp_list.append(ngram_minus_1_dict[0])
            ngrams_from_text += temp_list
        for ngram in ngrams_from_text:
            if (self.chars == False):
                tokens = [token for token in ngram.split(" ") if token != ""]
                ngram_size = len(tokens)
            else:
                ngram_size = len(ngram)
            count = self.model_dict_of_dicts[ngram_size][ngram] #check if the ngram is in the model_dict
            if count == 0:
                smooth_flag = True
        # if a laplace smooth is True:
        if smooth_flag:
            for item in ngrams_from_text:
                 log_w = self.smooth(item)  # calculates the likelihood prob , using smooth
                 log_prob += log_w
        else:
            for item in ngrams_from_text:

                  log_w = self.calculate_prob_for_evaluate(item)  # calculates the likelihood prob
                  log_prob += log_w

        return log_prob

    def calculate_prob_for_evaluate(self, ngram):
        """
            Returns the (log) probability of the specified ngram.

            Args:
                ngram (str): the ngram to calculate probability for.

            Returns:
                Float , probability.

            Explanations:
                For each case (words or chars) this function will calculate the (log) probability of the specified ngram.

        """

        if (self.chars == False): # when the case is words
            tokens = [token for token in ngram.split(" ") if token != ""] # split ngram to tokens
            ngram_size = len(tokens) # count how many word we have in the ngram
            if ngram_size < self.n: #for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    prob = (dict[ngram]) / (sum(self.model_dict_of_dicts[ngram_size+1].values())) # calculate probability
                    # print(math.log(prob))
                    return math.log(prob)
                else:
                    n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    c_i = dict[ngram]
                    prob = (c_i) / (dict_minus_1[n_minus_1_gram])
                    # print(math.log(prob))

                    return math.log(prob)
            #when the ngram size equal to n or bigger
            else:
                n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                # print(ngram)
                # print(n_minus_1_gram)
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
            c_i = self.model_dict[ngram]
            # print("the ci: ", c_i)
            # print(dict_minus_1)
            # print("the mehane: ",dict_minus_1[n_minus_1_gram] )
            prob = (c_i) / (dict_minus_1[n_minus_1_gram])
            # print("prob: ",prob)
            # print("log: ", math.log(prob))

            return math.log(prob)

        else: #in case of chars
            ngram_size = len(ngram) # count how many chars we have in the ngram
            if ngram_size < self.n: #for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    prob = (dict[ngram]) / (sum(self.model_dict_of_dicts[ngram_size+1].values())) # calculate probability
                    return math.log(prob)
                else:
                    char_minus_1_gram = ngram[0:len(ngram)-1]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    c_i = dict[ngram]
                    prob = (c_i) / (dict_minus_1[char_minus_1_gram])
                    return math.log(prob)

            # when the ngram size equal to n or bigger
            else:
                char_minus_1_gram = ngram[0:len(ngram) - 1]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]

            c_i = self.model_dict[ngram]
            # else, if not out of vocabulary:
            prob = c_i / dict_minus_1[char_minus_1_gram]
            return math.log(prob)

    def smooth(self, ngram):
        """
            Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.

            Explanation:
                this function calculate the probability the same like the calculate_prob_for_evaluate, but add 1 to the
                numerator and |v| for denominator according to laplace theory.
        """

        if (self.chars == False): # when the case is words
            tokens = [token for token in ngram.split(" ") if token != ""] # split ngram to tokens
            ngram_size = len(tokens) # count how many word we have in the ngram
            if ngram_size < self.n:
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    v = self.dict_of_vocabulary_count[ngram_size]
                    prob = (dict[ngram] + 1) / (v + sum(self.model_dict_of_dicts[ngram_size+1].values())) # calculate probability
                    return math.log(prob)

                else:
                    n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    v = len(dict.items())
                    c_i = dict[ngram]
                    prob = (c_i + 1) / (v + dict_minus_1[n_minus_1_gram])
                    return math.log(prob)

            else:
                n_minus_1_gram = ngram.rsplit(' ', 1)[0]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
            v = self.dict_of_vocabulary_count[self.n]
            c_i = self.model_dict[ngram]
            prob = (c_i + 1) / (dict_minus_1[n_minus_1_gram] + v)
            return math.log(prob)

        else:  # in case of chars
            ngram_size = len(ngram)  # count how many chars we have in the ngram
            if ngram_size < self.n:  # for calculate the prob for the start of the sentence (when its not ngram)
                dict = self.model_dict_of_dicts[ngram_size]
                if ngram_size == 1:
                    v = self.dict_of_vocabulary_count[ngram_size]
                    prob = (dict[ngram] + 1) / (v + sum(self.model_dict_of_dicts[ngram_size + 1].values()))  # calculate probability
                    return math.log(prob)
                else:
                    char_minus_1_gram = ngram[0:len(ngram) - 1]
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]
                    v = len(dict.items())
                    c_i = dict[ngram]
                    prob = (c_i + 1) / (v + dict_minus_1[char_minus_1_gram])
                    return math.log(prob)

            # when the ngram size equal to n or bigger
            else:
                char_minus_1_gram = ngram[0:len(ngram) - 1]
                if (self.n == 1):
                    dict_minus_1 = self.model_dict_of_dicts[1]
                else:
                    dict_minus_1 = self.model_dict_of_dicts[ngram_size - 1]

            v = self.dict_of_vocabulary_count[self.n]
            c_i = self.model_dict[ngram]
            prob = (c_i + 1) / (dict_minus_1[char_minus_1_gram] + v)
            return math.log(prob)


def normalize_text(text , chars_flag = False):
    """
       The function Returns a normalized version of the specified string.

      Args:
        text (str): the text to normalize
        chars_flag (boolean , defaults to False) : this flag help to know if we need to do a normalize to words or chars
                                                    i need to get it because this function is not in the class of N-gram.
      Returns:
        string. the normalized text.

      Explanation:
      replacement - replacement of special characters and spaces between lines in both cases (words and chars) with space
                    or in special case with "." when its an end of sentence. its help me to get sentences that is not
                    containing signs unrelated to the sentence.

      re.sub - Checks for each sign whether there is space before or after it. If not, add a space to make a separation.
    """
    #replacement of special characters and spaces between lines in both cases (words and chars)
    text = text.replace('\n', " ")
    text = text.replace('^', " ")
    text = text.replace('*', " ")
    text = text.replace('...', " ")
    text = text.replace('#', " ")
    text = text.replace('❤', " ")
    text = text.replace('\\', " ")
    text = text.replace('@', " ")
    text = text.replace("-", " ")
    text = text.replace('_', " ")
    text = text.replace('"', " ")
    text = text.replace('(', " ")
    text = text.replace(')', " ")
    text = text.replace(';', " ")
    text = text.replace('<s>', ".")

    if chars_flag == False: #case of words
        lower_text = text.lower()  # Turns all text into small letters
        norm_text = re.sub('(?<! )(?=[".,!@#^&*?/<>:+-])|(?<=[".,!@#^&*?/:<>+-])(?! )', r' ', lower_text)
        return norm_text
    else: #case of chars
        text = ' '.join(text.split())
        norm_text = text.lower()
        return norm_text


def who_am_i():
    """
        Returns a dictionary with my name, id and email.
        keys=['name', 'id','email']
    """
    return {'name': 'Omer Keidar', 'id': '307887984', 'email': 'omerkei@post.bgu.ac.il'}


